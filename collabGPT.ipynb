{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Madhusudan3223/CollabGPT/blob/main/collabGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¤– CollabGPT: A Multi-PDF RAG Chatbot using Cohere Embeddings\n",
        "\n",
        "Welcome to **CollabGPT** â€” a powerful Retrieval-Augmented Generation (RAG) chatbot built with **Cohereâ€™s embedding models**. This notebook lets you upload multiple PDF documents and ask natural language questions about them using a Gradio-powered chatbot interface.\n",
        "\n",
        "### ğŸš€ What This Notebook Does:\n",
        "1. ğŸ“„ Upload multiple PDFs (research papers, reports, etc.)\n",
        "2. ğŸ” Extract and split text into meaningful chunks\n",
        "3. ğŸ§  Generate embeddings using **Cohereâ€™s `embed-english-v3.0` model**\n",
        "4. ğŸ” Match your query with the most relevant chunks using cosine similarity\n",
        "5. ğŸ’¬ Use the matched content as context to answer your questions\n",
        "6. ğŸ§‘â€ğŸ’» Interact with a **Gradio UI** to ask anything from your documents\n",
        "\n",
        "### ğŸ“š Use Case Examples:\n",
        "- Summarize PDF content\n",
        "- Ask questions like â€œWhat are the key findings?â€\n",
        "- Extract definitions or bullet points from dense documents\n",
        "- Academic literature review\n",
        "- Product or policy document Q&A\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xUbRdkG1CivR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ“¦ Step 1: Install Required Libraries\n",
        "Before running the chatbot, we need to install the following Python packages:\n",
        "\n",
        "cohere==4.44: To use Cohereâ€™s embedding and text generation APIs\n",
        "\n",
        "PyMuPDF: For extracting text from PDF files"
      ],
      "metadata": {
        "id": "MSqQF8TBDgjI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M8NVW1oQTPz",
        "outputId": "16794a2d-bd96-4316-91bd-f83fe0e49e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cohere==4.44 in /usr/local/lib/python3.11/dist-packages (4.44)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from cohere==4.44) (3.11.15)\n",
            "Requirement already satisfied: backoff<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from cohere==4.44) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2.0,>=1.8 in /usr/local/lib/python3.11/dist-packages (from cohere==4.44) (1.11.1)\n",
            "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from cohere==4.44) (6.11.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.11/dist-packages (from cohere==4.44) (2.32.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from cohere==4.44) (2.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.44) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.44) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.44) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.44) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.44) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.44) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0,>=3.0->cohere==4.44) (1.20.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata<7.0,>=6.0->cohere==4.44) (3.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.25.0->cohere==4.44) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.25.0->cohere==4.44) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.25.0->cohere==4.44) (2025.6.15)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.1.2->aiohttp<4.0,>=3.0->cohere==4.44) (4.14.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install cohere==4.44 PyMuPDF"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“„  Upload  PDF Documents\n",
        "\n",
        "Use the cell below to upload multiple PDFs into the notebook. These PDFs will act as the knowledge source for the chatbot.\n",
        "\n",
        "- Files are automatically saved into a `data/` folder.\n",
        "- Make sure the uploaded PDFs are relevant to your domain or use case.\n"
      ],
      "metadata": {
        "id": "YQHR4_6BDtqg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "id": "lGE63XvQQaQe",
        "outputId": "0fea4dd8-317e-470b-c2d3-1c7c5ae5057c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e27aec75-9228-4c0d-b81e-56ea3467be6e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e27aec75-9228-4c0d-b81e-56ea3467be6e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving AutoGPT+P.pdf to AutoGPT+P.pdf\n",
            "Saving AUTO GPT.pdf to AUTO GPT.pdf\n",
            "Saving Large Language Models Survey.pdf to Large Language Models Survey.pdf\n",
            "Saving Large Language Model.pdf to Large Language Model.pdf\n",
            "Saving PROMPT DESIGN.pdf to PROMPT DESIGN.pdf\n",
            "Saving Prompt Report.pdf to Prompt Report.pdf\n",
            "Saving Prompt Engineering.pdf to Prompt Engineering.pdf\n",
            "Saving GEN AI.pdf to GEN AI.pdf\n",
            "Saving RAG.pdf to RAG.pdf\n",
            "Saving Retrieval-Augmented Generation for Large.pdf to Retrieval-Augmented Generation for Large.pdf\n",
            "Saving LLm2.pdf to LLm2.pdf\n",
            "Saving LLM.pdf to LLM.pdf\n",
            "Saving Low-resolution-25Oct2024-Conversations-for-tomorrow_Edition_9_Report-V2-1.pdf to Low-resolution-25Oct2024-Conversations-for-tomorrow_Edition_9_Report-V2-1.pdf\n",
            "Saving ai2.pdf to ai2.pdf\n",
            "Saving ai1.pdf to ai1.pdf\n",
            "âœ… Text extraction complete. Preview:\n",
            "LLM Multi-Agent Systems: Challenges and Open Problems\n",
            "Shanshan Han 1 Qifan Zhang 1 Yuhang Yao 2 Weizhao Jin 3 Zhaozhuo Xu 4\n",
            "Abstract\n",
            "This paper explores multi-agent systems and\n",
            "identify challenges that remain inadequately ad-\n",
            "dressed. By leveraging the diverse capabilities\n",
            "and roles of individual agents, multi-agent sys-\n",
            "tems can tackle complex tasks through agent col-\n",
            "laboration. We discuss optimizing task alloca-\n",
            "tion, fostering robust reasoning through iterative\n",
            "debates, managing complex and layered context\n",
            "information, and enhancing memory management\n",
            "to support the intricate interactions within multi-\n",
            "agent systems. We also explore potential appli-\n",
            "cations of multi-agent systems in blockchain sys-\n",
            "tems to shed light on their future development and\n",
            "application in real-world distributed systems.\n",
            "1. Introduction\n",
            "Multi-agent systems enhance the capabilities of single LLM\n",
            "agents by leveraging collaborations among agents and their\n",
            "specialized abilities (Talebirad & Nadiri, 2023; Zhang et\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Create folder\n",
        "os.makedirs(\"pdfs\", exist_ok=True)\n",
        "\n",
        "# Upload PDFs\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move them to 'pdfs/' folder\n",
        "for filename in uploaded.keys():\n",
        "    os.rename(filename, f\"pdfs/{filename}\")\n",
        "\n",
        "# Function to extract text from all PDFs\n",
        "def extract_all_text(pdf_folder=\"pdfs\"):\n",
        "    all_text = \"\"\n",
        "    for filename in os.listdir(pdf_folder):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            path = os.path.join(pdf_folder, filename)\n",
        "            with fitz.open(path) as doc:\n",
        "                for page in doc:\n",
        "                    all_text += page.get_text()\n",
        "    return all_text\n",
        "\n",
        "# Extract and show first few lines\n",
        "raw_text = extract_all_text()\n",
        "print(\"âœ… Text extraction complete. Preview:\")\n",
        "print(raw_text[:1000])  # Preview first 1000 characters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“š  Load and Split PDF Text into Chunks\n",
        "\n",
        "In this step:\n",
        "\n",
        "- We use `PyMuPDFLoader` to extract text from each uploaded PDF.\n",
        "- Then, we split the text into smaller, overlapping chunks using `RecursiveCharacterTextSplitter`.\n",
        "\n",
        "This chunking makes it easier for the language model to process and embed meaningful context.\n"
      ],
      "metadata": {
        "id": "6gW0fCcXEC03"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0irLM5TQzQN",
        "outputId": "1f9690f0-77da-4452-ca39-e8c844d8991f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Split into 4352 chunks.\n",
            "ğŸ“¦ Embedding batch 1 of 44...\n",
            "ğŸ“¦ Embedding batch 2 of 44...\n",
            "ğŸ“¦ Embedding batch 3 of 44...\n",
            "ğŸ“¦ Embedding batch 4 of 44...\n",
            "ğŸ“¦ Embedding batch 5 of 44...\n",
            "ğŸ“¦ Embedding batch 6 of 44...\n",
            "ğŸ“¦ Embedding batch 7 of 44...\n",
            "ğŸ“¦ Embedding batch 8 of 44...\n",
            "ğŸ“¦ Embedding batch 9 of 44...\n",
            "ğŸ“¦ Embedding batch 10 of 44...\n",
            "ğŸ“¦ Embedding batch 11 of 44...\n",
            "ğŸ“¦ Embedding batch 12 of 44...\n",
            "ğŸ“¦ Embedding batch 13 of 44...\n",
            "ğŸ“¦ Embedding batch 14 of 44...\n",
            "ğŸ“¦ Embedding batch 15 of 44...\n",
            "ğŸ“¦ Embedding batch 16 of 44...\n",
            "ğŸ“¦ Embedding batch 17 of 44...\n",
            "ğŸ“¦ Embedding batch 18 of 44...\n",
            "ğŸ“¦ Embedding batch 19 of 44...\n",
            "ğŸ“¦ Embedding batch 20 of 44...\n",
            "ğŸ“¦ Embedding batch 21 of 44...\n",
            "ğŸ“¦ Embedding batch 22 of 44...\n",
            "ğŸ“¦ Embedding batch 23 of 44...\n",
            "ğŸ“¦ Embedding batch 24 of 44...\n",
            "ğŸ“¦ Embedding batch 25 of 44...\n",
            "ğŸ“¦ Embedding batch 26 of 44...\n",
            "ğŸ“¦ Embedding batch 27 of 44...\n",
            "ğŸ“¦ Embedding batch 28 of 44...\n",
            "ğŸ“¦ Embedding batch 29 of 44...\n",
            "ğŸ“¦ Embedding batch 30 of 44...\n",
            "ğŸ“¦ Embedding batch 31 of 44...\n",
            "ğŸ“¦ Embedding batch 32 of 44...\n",
            "ğŸ“¦ Embedding batch 33 of 44...\n",
            "ğŸ“¦ Embedding batch 34 of 44...\n",
            "ğŸ“¦ Embedding batch 35 of 44...\n",
            "ğŸ“¦ Embedding batch 36 of 44...\n",
            "ğŸ“¦ Embedding batch 37 of 44...\n",
            "ğŸ“¦ Embedding batch 38 of 44...\n",
            "ğŸ“¦ Embedding batch 39 of 44...\n",
            "ğŸ“¦ Embedding batch 40 of 44...\n",
            "ğŸ“¦ Embedding batch 41 of 44...\n",
            "ğŸ“¦ Embedding batch 42 of 44...\n",
            "ğŸ“¦ Embedding batch 43 of 44...\n",
            "ğŸ“¦ Embedding batch 44 of 44...\n",
            "âœ… All embeddings created!\n"
          ]
        }
      ],
      "source": [
        "import cohere\n",
        "import os\n",
        "import time\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set your API key using Colab Secrets Manager\n",
        "# Add my Cohere API key to the Secrets Manager with the name 'COHERE_API_KEY'\n",
        "co = cohere.Client(userdata.get('COHERE_API_KEY'))\n",
        "\n",
        "# Chunk text into overlapping pieces\n",
        "def chunk_text(text, chunk_size=500, overlap=100):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "# Chunk the raw text\n",
        "chunks = chunk_text(raw_text)\n",
        "print(f\"âœ… Split into {len(chunks)} chunks.\")\n",
        "\n",
        "# Embed chunks using Cohere with batching\n",
        "all_embeddings = []\n",
        "batch_size = 100\n",
        "\n",
        "for i in range(0, len(chunks), batch_size):\n",
        "    batch = chunks[i:i + batch_size]\n",
        "    print(f\"ğŸ“¦ Embedding batch {i // batch_size + 1} of {len(chunks) // batch_size + 1}...\")\n",
        "\n",
        "    response = co.embed(\n",
        "        texts=batch,\n",
        "        model=\"embed-english-v3.0\",\n",
        "        input_type=\"search_document\"\n",
        "    )\n",
        "    all_embeddings.extend(response.embeddings)\n",
        "\n",
        "    # Pause to respect rate limit\n",
        "    if i + batch_size < len(chunks):\n",
        "        time.sleep(60)\n",
        "\n",
        "print(\"âœ… All embeddings created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ¤–: Generate Embeddings with Cohere\n",
        "\n",
        "We use Cohereâ€™s `embed-english-v3.0` model to convert each document chunk into a dense vector. These vectors are crucial for semantic search and answering user queries.\n"
      ],
      "metadata": {
        "id": "VYaS2EN2FwlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Vector search: get top N similar chunks using cosine similarity\n",
        "def find_similar_chunks(query, all_embeddings, chunks, co, k=5):\n",
        "    # Embed the query\n",
        "    query_embed = co.embed(\n",
        "        texts=[query],\n",
        "        model=\"embed-english-v3.0\",\n",
        "        input_type=\"search_query\"\n",
        "    ).embeddings[0]\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarities = []\n",
        "    for i, emb in enumerate(all_embeddings):\n",
        "        sim = np.dot(query_embed, emb) / (np.linalg.norm(query_embed) * np.linalg.norm(emb))\n",
        "        similarities.append((i, sim))\n",
        "\n",
        "    # Sort and select top k\n",
        "    top_k = sorted(similarities, key=lambda x: x[1], reverse=True)[:k]\n",
        "    return [chunks[i] for i, _ in top_k]\n",
        "\n",
        "# Generate answer using top-k context\n",
        "def generate_answer(query, context_chunks, co):\n",
        "    context = \"\\n\".join(context_chunks)\n",
        "    message = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "\n",
        "    response = co.chat(\n",
        "        message=message,\n",
        "        temperature=0.3,\n",
        "        model=\"command-r\" # Ensure correct model for chat\n",
        "    )\n",
        "    return response.text.strip()"
      ],
      "metadata": {
        "id": "IwmTpNMh2P-a"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ§  : Generate Answer using Cohere LLM\n",
        "This function takes the user query and the most relevant document chunks (retrieved earlier), and sends them to Cohereâ€™s generate API."
      ],
      "metadata": {
        "id": "nw6SWr4hxxSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query, context_chunks, co):\n",
        "    context = \"\\n\".join(context_chunks)\n",
        "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "\n",
        "    response = co.generate(\n",
        "        model=\"command\",  # âœ… Corrected model name\n",
        "        prompt=prompt,\n",
        "        max_tokens=300,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    return response.generations[0].text.strip()\n"
      ],
      "metadata": {
        "id": "NKaUyomlqwVy"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ’¬ : Generate Contextual Answer with co.chat()\n",
        "This function is responsible for generating an answer to the userâ€™s question based on the most relevant chunks from your uploaded PDFs. It works like this:\n",
        "\n",
        "âœ… Joins the top k relevant chunks into a single context block\n",
        "\n",
        "âœ… Builds a prompt in the format:"
      ],
      "metadata": {
        "id": "cx74bvQSx_ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query, context_chunks, co):\n",
        "    context = \"\\n\".join(context_chunks)\n",
        "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "\n",
        "    response = co.chat(\n",
        "        message=prompt,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    return response.text.strip()\n"
      ],
      "metadata": {
        "id": "Avh7g0Zora7O"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ§  : Ask Questions & Generate Answers (RAG Inference)\n",
        "This is the main inference pipeline where the chatbot answers your question by combining semantic search and text generation, powered by Cohere:\n",
        "\n",
        "âœ… Query input â€” You ask a natural language question.\n",
        "\n",
        "âœ… Step 1: Semantic search â€” The find_similar_chunks function uses the Cohere embeddings and FAISS-like search to retrieve the top k most relevant chunks from the uploaded PDFs.\n",
        "\n",
        "âœ… Step 2: Contextual generation â€” These chunks and the query are passed to the generate_answer function, which uses co.chat() to produce a smart, context-aware answer.\n",
        "\n",
        "âœ… Step 3: Print the result â€” The final answer is displayed."
      ],
      "metadata": {
        "id": "ikRJpmxdFy6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask your question here\n",
        "query = \"Summarize the key trends in the Conversations for Tomorrow report.\"\n",
        "\n",
        "# Step 1: Find similar chunks\n",
        "top_chunks = find_similar_chunks(query, all_embeddings, chunks, co, k=5)\n",
        "\n",
        "# Step 2: Generate the answer\n",
        "answer = generate_answer(query, top_chunks, co)\n",
        "\n",
        "# Step 3: Show result\n",
        "print(\"ğŸ“Œ Question:\", query)\n",
        "print(\"\\nğŸ§  Answer:\\n\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnrNXOh42Dgx",
        "outputId": "0396415f-b2d6-464d-964b-1261891f20f6"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Œ Question: Summarize the key trends in the Conversations for Tomorrow report.\n",
            "\n",
            "ğŸ§  Answer:\n",
            " The **Conversations for Tomorrow** report highlights several key trends and insights related to the intersection of generative AI (Gen AI), sustainability, and the future of business and society. Hereâ€™s a summary of the key trends:\n",
            "\n",
            "1. **Rapid Adoption of Gen AI**: Organizations globally are rapidly embedding Gen AI across various functions, creating a ripple effect for wider societal impact.  \n",
            "2. **Carbon Footprint Concerns**: Over one-third of organizations are already tracking their Gen AI carbon emissions, acknowledging the significant environmental impact of AI technologies.  \n",
            "3. **Focus on Sustainability**: The report emphasizes the dual transition to a digital and sustainable economy, with a spotlight on sustainability and climate tech innovations.  \n",
            "4. **Leadership and Innovation**: Gen AI is being leveraged for leadership and innovation, with a focus on uncovering innovations that matter in the eco-digital era.  \n",
            "5. **Strategic Imperatives**: The Capgemini Research Institute identifies strategic imperatives for the future of business and society, driven by Gen AI and sustainability.  \n",
            "6. **Multidisciplinary Collaboration**: Insights are distilled from leaders in global business, academia, startups, and wider society, fostering a holistic perspective on the future.  \n",
            "7. **Creative Exuberance**: Gen AI is described as being at the threshold of an outburst of creative exuberance, symbolizing the myriad possibilities at the intersection of light and shadow.  \n",
            "\n",
            "Overall, the report underscores the transformative potential of Gen AI while addressing its challenges, particularly in sustainability and environmental impact.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ’¬ : Generate Answers using co.chat() with the \"command-r\" Model\n",
        "This function uses Cohere's chat() endpoint to generate smart, contextual answers from the top retrieved chunks.\n",
        "\n",
        "Key actions:\n",
        "\n",
        "ğŸ§© Combine context chunks into a unified string block.\n",
        "\n",
        "ğŸ§  Build a message prompt with the format:"
      ],
      "metadata": {
        "id": "2k4ySoSlGyEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query, context_chunks, co):\n",
        "    context = \"\\n\".join(context_chunks)\n",
        "    message = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "\n",
        "    response = co.chat(\n",
        "        message=message,\n",
        "        temperature=0.3,\n",
        "        model=\"command-r\" # Ensure correct model for chat\n",
        "    )\n",
        "    return response.text.strip()"
      ],
      "metadata": {
        "id": "NS1tJpb1rWPD"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ” : Interactive Q&A Loop (Console-based)\n",
        "This loop enables users to interactively ask unlimited questions in the terminal. It retrieves relevant context and generates answers in real-time using Cohere.\n",
        "\n",
        "How it works:\n",
        "\n",
        "ğŸ’¬ Prompts the user to input a question (or type 'exit' to stop).\n",
        "\n",
        "ğŸ” Uses find_similar_chunks() to get top-k relevant document chunks based on semantic similarity.\n",
        "\n",
        "ğŸ§  Sends the query + context to generate_answer() using Cohere's chat() API.\n",
        "\n",
        "ğŸ–¨ï¸ Displays the AI-generated answer in the console.\n",
        "\n",
        "â™»ï¸ Repeats until the user types 'exit'."
      ],
      "metadata": {
        "id": "MICPeDirKQNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'while True:\n",
        "    query = input(\"ğŸ” Ask a question (or type 'exit' to quit): \")\n",
        "    if query.lower() == 'exit':\n",
        "        print(\"ğŸ‘‹ Exiting Q&A session.\")\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        top_chunks = find_similar_chunks(query, all_embeddings, chunks, co, k=5)\n",
        "        answer = generate_answer(query, top_chunks, co)\n",
        "        print(\"\\nğŸ§  Answer:\\n\", answer)\n",
        "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸ Error:\", str(e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mw0lYirV3jbU",
        "outputId": "13096c8b-4061-4893-b5fc-6dda99625102"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Ask a question (or type 'exit' to quit): hi\n",
            "\n",
            "ğŸ§  Answer:\n",
            " Hello! I hope you're doing well today. Is there a specific question you'd like to ask regarding the text provided? It seems like a collection of references related to AI and natural language processing.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "ğŸ” Ask a question (or type 'exit' to quit): exit\n",
            "ğŸ‘‹ Exiting Q&A session.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ§  : Interactive Terminal-Based Q&A + History Logger\n",
        "This block enables a continuous chat loop in the terminal, allowing you to interactively ask questions and get answers from your uploaded PDFs using Cohere embeddings + generation.\n",
        "\n",
        "ğŸ”„ Key Features:\n",
        "ğŸ’¬ Ask Anything: User can input any question from the console.\n",
        "\n",
        "ğŸ§  Find Top Relevant Chunks: Uses semantic search via find_similar_chunks() to pull matching content.\n",
        "\n",
        "ğŸ¤– Generate Accurate Answers: Feeds the context to generate_answer() (powered by Cohere's chat() API).\n",
        "\n",
        "ğŸ“œ Log Q&A: Every exchange is saved in the qa_history list for reference or export."
      ],
      "metadata": {
        "id": "YM4_TV-XzDxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_history = []\n",
        "\n",
        "while True:\n",
        "    query = input(\"ğŸ” Ask a question (or type 'exit' to quit): \")\n",
        "    if query.lower() == 'exit':\n",
        "        print(\"ğŸ‘‹ Exiting Q&A session.\")\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        top_chunks = find_similar_chunks(query, all_embeddings, chunks, co, k=5)\n",
        "        answer = generate_answer(query, top_chunks, co)\n",
        "        print(\"\\nğŸ§  Answer:\\n\", answer)\n",
        "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "\n",
        "        # Save each Q&A to history\n",
        "        qa_history.append(f\"Question: {query}\\nAnswer: {answer}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸ Error:\", str(e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OiJT-3z4RyZ",
        "outputId": "addba1a2-130d-43a9-d02f-7610465ce53b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Ask a question (or type 'exit' to quit): hi\n",
            "\n",
            "ğŸ§  Answer:\n",
            " Hello! I hope you're doing well today. Is there a specific question you'd like to ask regarding the text provided? It seems like a research article or a summary of various studies related to natural language processing and interactive systems.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "ğŸ” Ask a question (or type 'exit' to quit): exit\n",
            "ğŸ‘‹ Exiting Q&A session.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ’¾: Save Q&A History to a Text File\n",
        "This optional step allows you to export the entire Q&A session (from the interactive loop) to a .txt file. Each question and answer pair stored in the qa_history list is saved line-by-line for later reference, sharing, or reporting."
      ],
      "metadata": {
        "id": "j-wmmsXzzSrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to text file\n",
        "with open(\"qa_history.txt\", \"w\") as f:\n",
        "    f.writelines([qa + \"\\n\" for qa in qa_history])\n",
        "\n",
        "print(\"âœ… Q&A history saved to 'qa_history.txt'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNoDuPg04SQh",
        "outputId": "8c2a38c8-42a9-49e7-f383-b2cca2f5a100"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Q&A history saved to 'qa_history.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸŒ : Install Gradio for Web UI\n",
        "We'll now install Gradio, a lightweight Python library to build a web-based chatbot interface. This makes it easy to interact with the RAG system directly in your browserâ€”no need to use the terminal."
      ],
      "metadata": {
        "id": "PbDWj9krzcCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet\n"
      ],
      "metadata": {
        "id": "lTonFjAQ4XG8"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ§  : Launch Chatbot with Gradio Web UI\n",
        "Now letâ€™s wrap our PDF QA system in a user-friendly chatbot interface using Gradio. This allows you to:\n",
        "\n",
        "Interact with your Cohere-powered chatbot in the browser\n",
        "\n",
        "Ask natural language questions about your uploaded PDFs\n",
        "\n",
        "Share your app via a public link (if share=True is enabled)"
      ],
      "metadata": {
        "id": "WHxTg4nWztDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Define chatbot function\n",
        "def chatbot_interface(query):\n",
        "    try:\n",
        "        # Ensure these variables are accessible in the global scope or passed\n",
        "        # For Gradio, it's often easiest if they are global after running setup cells\n",
        "        global all_embeddings, chunks, co\n",
        "        top_chunks = find_similar_chunks(query, all_embeddings, chunks, co, k=5)\n",
        "        answer = generate_answer(query, top_chunks, co)\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        return f\"âš ï¸ Error: {str(e)}\"\n",
        "\n",
        "# Launch Gradio UI\n",
        "gr.Interface(\n",
        "    fn=chatbot_interface,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask me anything from the PDFs...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"ğŸ“š CollabGPT\",\n",
        "    description=\"Ask questions based on your uploaded PDFs. Powered by Cohere embeddings + RAG.\",\n",
        "    theme=\"soft\"\n",
        ").launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "CiJ8XI9X4sC6",
        "outputId": "2b7212c1-df26-4019-bef9-f211f336a99c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7fba9f608539673f23.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7fba9f608539673f23.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RHoWHxEx4w_V"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnzuNpBDPoMpYZTwJOkwX1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
